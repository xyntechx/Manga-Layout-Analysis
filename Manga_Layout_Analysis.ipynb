{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Manga Layout Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialisation"
      ],
      "metadata": {
        "id": "TvSDlr5yPqyU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip3 uninstall keras-nightly\n",
        "!pip3 uninstall -y tensorflow\n",
        "!pip3 install keras==2.1.6\n",
        "!pip3 install tensorflow==1.15.0\n",
        "!pip3 install h5py==2.10.0"
      ],
      "outputs": [],
      "metadata": {
        "id": "TKJV3yaVMRhe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!git clone https://github.com/matterport/Mask_RCNN.git"
      ],
      "outputs": [],
      "metadata": {
        "id": "3oztxkItMb67"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%cd Mask_RCNN"
      ],
      "outputs": [],
      "metadata": {
        "id": "Wa61M3fOMhdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!python setup.py install\n",
        "!pip install -r requirements.txt"
      ],
      "outputs": [],
      "metadata": {
        "id": "sNR9tCIWMmNf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install elementpath\n",
        "!pip install manga109api"
      ],
      "outputs": [],
      "metadata": {
        "id": "GVw_z1VjO1nb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import elementpath\n",
        "from xml.etree import ElementTree\n",
        "import manga109api\n",
        "from google.colab import files\n",
        "from os import listdir\n",
        "from numpy import zeros, asarray, expand_dims, mean\n",
        "from numpy import asarray\n",
        "from mrcnn.utils import Dataset, extract_bboxes, compute_ap\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.visualize import display_instances\n",
        "from mrcnn.model import MaskRCNN, load_image_gt, mold_image\n",
        "import matplotlib.pyplot as pyplot\n",
        "from matplotlib.patches import Rectangle, Arrow\n",
        "import math"
      ],
      "outputs": [],
      "metadata": {
        "id": "Fq7ge8sqO2kB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "root_dir = \"/content/drive/MyDrive/NRP/Project/Manga109/\"\n",
        "p = manga109api.Parser(root_dir=root_dir)"
      ],
      "outputs": [],
      "metadata": {
        "id": "_jIldJh4T9fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reformat Manga109 annotations"
      ],
      "metadata": {
        "id": "1i5QUXQDPboM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# %cd /content\n",
        "# for book in p.books:\n",
        "#   tree = ElementTree.parse(root_dir + \"annotations/\" + book + \".xml\")\n",
        "#   root = tree.getroot()\n",
        "\n",
        "#   %mkdir $book\n",
        "#   %cd /content/$book\n",
        "\n",
        "#   for page in root.findall('.//page'):\n",
        "#     new_xml = page\n",
        "#     b_xml = ElementTree.tostring(new_xml)\n",
        "#     with open(\"new_\" + book + str(page.attrib[\"index\"]) + \".xml\", \"wb\") as f:\n",
        "#       f.write(b_xml)\n",
        "  \n",
        "#   %cd /content"
      ],
      "outputs": [],
      "metadata": {
        "id": "yjmrNZ3GXLWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# for book in p.books:\n",
        "#   !zip -r /content/$book /content/$book"
      ],
      "outputs": [],
      "metadata": {
        "id": "g90nU93IYLqo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# print('\\n'.join(p.books))"
      ],
      "outputs": [],
      "metadata": {
        "id": "6qfL7lwheT6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Dataset"
      ],
      "metadata": {
        "id": "BwQ9GyH4QKmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class MangaDataset(Dataset):\n",
        "  def load_dataset(self, is_train=True):\n",
        "    self.add_class(\"dataset\", 1, \"face\")\n",
        "    self.add_class(\"dataset\", 2, \"text\")\n",
        "\n",
        "    for book in p.books:\n",
        "      images_dir = root_dir + \"images/\" + book + \"/\"\n",
        "      annotations_dir = root_dir + \"/annotations/\" + book + \"/\"\n",
        "\n",
        "      for img in listdir(images_dir):\n",
        "        image_id = img[:-4]\n",
        "      \n",
        "        tree = ElementTree.parse(annotations_dir + \"new_\" + book + str(int(image_id)) + \".xml\")\n",
        "        root = tree.getroot()\n",
        "        faces = []\n",
        "        texts = []\n",
        "\n",
        "        for face in root.findall(\".//face\"):\n",
        "          faces.append(face)\n",
        "        \n",
        "        for text in root.findall(\".//text\"):\n",
        "          texts.append(text)\n",
        "        \n",
        "        if len(faces) < 1: #if there are no faces\n",
        "          continue\n",
        "        \n",
        "        if len(texts) < 1: #if there are no texts\n",
        "          continue\n",
        " \n",
        "        if is_train and int(image_id) >= 50:\n",
        "          continue\n",
        "\n",
        "        if not is_train and int(image_id) < 50:\n",
        "          continue\n",
        "        \n",
        "        img_path = images_dir + img\n",
        "        ann_path = annotations_dir + \"new_\" + book + str(int(image_id)) + \".xml\"\n",
        "\n",
        "        self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path, class_ids = [0,1,2])\n",
        "\n",
        "\n",
        "  def extract_boxes(self, filename):\n",
        "    tree = ElementTree.parse(filename)\n",
        "    root = tree.getroot()\n",
        "    boxes = []\n",
        "\n",
        "    for box in root.findall(\".//face\"):\n",
        "      att = box.attrib\n",
        "      xmin = att['xmin']\n",
        "      ymin = att['ymin']\n",
        "      xmax = att['xmax']\n",
        "      ymax = att['ymax']\n",
        "      coors = [xmin, ymin, xmax, ymax, \"face\"]\n",
        "      boxes.append(coors)\n",
        "    \n",
        "    for box in root.findall(\".//text\"):\n",
        "      att = box.attrib\n",
        "      xmin = att['xmin']\n",
        "      ymin = att['ymin']\n",
        "      xmax = att['xmax']\n",
        "      ymax = att['ymax']\n",
        "      coors = [xmin, ymin, xmax, ymax, \"text\"]\n",
        "      boxes.append(coors)\n",
        "\n",
        "    page_att = root.attrib\n",
        "    width = int(page_att['width'])\n",
        "    height = int(page_att['height'])\n",
        "\n",
        "    return boxes, width, height\n",
        "\n",
        "\n",
        "  def load_mask(self, image_id):\n",
        "    info = self.image_info[image_id]\n",
        "    path = info[\"annotation\"]\n",
        "    boxes, w, h = self.extract_boxes(path)\n",
        "    \n",
        "    masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
        "\n",
        "    class_ids = []\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "      box = boxes[i]\n",
        "      row_s, row_e = box[1], box[3]\n",
        "      col_s, col_e = box[0], box[2]\n",
        "\n",
        "      if box[4] == \"face\":\n",
        "        masks[int(row_s):int(row_e), int(col_s):int(col_e), i] = 1\n",
        "        class_ids.append(self.class_names.index('face'))\n",
        "\n",
        "      elif box[4] == \"text\":\n",
        "        masks[int(row_s):int(row_e), int(col_s):int(col_e), i] = 2\n",
        "        class_ids.append(self.class_names.index('text'))\n",
        "\n",
        "    return masks, asarray(class_ids, dtype='int32')\n",
        "\n",
        "\n",
        "  def image_reference(self, image_id):\n",
        "    info = self.image_info[image_id]\n",
        "    return info[\"path\"]"
      ],
      "outputs": [],
      "metadata": {
        "id": "uTO-V6wgDDET"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# train set\n",
        "# train_set = MangaDataset()\n",
        "# train_set.load_dataset(is_train=True)\n",
        "# train_set.prepare()\n",
        "# print('Train: %d' % len(train_set.image_ids))\n",
        " \n",
        "# test/val set\n",
        "test_set = MangaDataset()\n",
        "test_set.load_dataset(is_train=False)\n",
        "test_set.prepare()\n",
        "print('Test: %d' % len(test_set.image_ids))"
      ],
      "outputs": [],
      "metadata": {
        "id": "GQY5a2ZtVZd0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load an image and mask\n",
        "image_id = 1\n",
        "image = train_set.load_image(image_id)\n",
        "print(image.shape)\n",
        "\n",
        "mask, class_ids = train_set.load_mask(image_id)\n",
        "print(mask.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "suboEjk0mTvL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# display image with masks and bounding boxes\n",
        "bbox = extract_bboxes(mask)\n",
        "display_instances(image, bbox, mask, class_ids, train_set.class_names)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bf9LK6YXgA4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "5Kxl7xZvfVRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class MangaConfig(Config):\n",
        "  NAME = \"manga_cfg\"\n",
        "  NUM_CLASSES = 1 + 2\n",
        "  STEPS_PER_EPOCH = 131"
      ],
      "outputs": [],
      "metadata": {
        "id": "6iCDgc7KfYpw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "config = MangaConfig()\n",
        "model = MaskRCNN(mode='training', model_dir='/content', config=config)\n",
        "# model.load_weights('/content/drive/MyDrive/NRP/Project/mask_rcnn_coco.h5',\n",
        "#                    by_name=True,\n",
        "#                    exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/NRP/Project/FaceSpeech/model_2.h5',\n",
        "                   by_name=True,\n",
        "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "\n",
        "model.train(train_set, test_set, learning_rate=0.00001, epochs=40, layers=\"all\")\n",
        "\n",
        "# model naming: [classes]_[learning_rate]_[epochs]_[layers]\n",
        "\n",
        "# LEARNING_RATE = 0.001\n",
        "\n",
        "# We could follow this training with further epochs that fine-tune all of the weights in the model.\n",
        "# This could be achieved by using a smaller learning rate and changing the ‘layer’ argument from ‘heads’ to ‘all’.\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "rMDpJv8dgCfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model"
      ],
      "metadata": {
        "id": "_esT8GtjQnpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class PredictionConfig(Config):\n",
        "  NAME = \"manga_cfg\"\n",
        "  NUM_CLASSES = 1 + 2\n",
        "  GPU_COUNT = 1\n",
        "  IMAGES_PER_GPU = 1"
      ],
      "outputs": [],
      "metadata": {
        "id": "sKR7ueWM41Yj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cfg = PredictionConfig()\n",
        "model = MaskRCNN(mode='inference', model_dir='/content', config=cfg)"
      ],
      "outputs": [],
      "metadata": {
        "id": "DGwpuPD045Gw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.load_weights('/content/drive/MyDrive/NRP/Project/FaceSpeech/model_2.h5', by_name=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "35C0dWQC4-bR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def evaluate_model(dataset, model, cfg):\n",
        "  APs = []\n",
        "  for image_id in dataset.image_ids:\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)\n",
        "    scaled_image = mold_image(image, cfg)\n",
        "    sample = expand_dims(scaled_image, 0)\n",
        "    yhat = model.detect(sample, verbose=0)\n",
        "    r = yhat[0]\n",
        "\n",
        "    AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "    APs.append(AP)\n",
        "\n",
        "  mAP = mean(APs)\n",
        "  return mAP"
      ],
      "outputs": [],
      "metadata": {
        "id": "3e2b_whw5Rqj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# evaluate model on training dataset\n",
        "# train_mAP = evaluate_model(train_set, model, cfg)\n",
        "# print(\"Train mAP: %.3f\" % train_mAP)\n",
        "\n",
        "# evaluate model on test dataset\n",
        "test_mAP = evaluate_model(test_set, model, cfg)\n",
        "print(\"Test mAP: %.3f\" % test_mAP)"
      ],
      "outputs": [],
      "metadata": {
        "id": "d3fmfY2Q5VFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Associate Face to Text"
      ],
      "metadata": {
        "id": "ji8U486_kHF7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def arrow_face_text(dataset, image_id, face_to_nearest_text):\n",
        "  image = dataset.load_image(image_id)\n",
        "  mask, _ = dataset.load_mask(image_id)\n",
        "  scaled_image = mold_image(image, cfg)\n",
        "  sample = expand_dims(scaled_image, 0)\n",
        "\n",
        "  yhat = model.detect(sample, verbose=0)[0]\n",
        "\n",
        "  pyplot.subplot(1, 2, image_id*2+1)\n",
        "  pyplot.imshow(image)\n",
        "  pyplot.title('Face to Text')\n",
        "\n",
        "  for j in range(mask.shape[2]):\n",
        "    pyplot.imshow(mask[:, :, j], cmap='gray', alpha=0.3)\n",
        "\n",
        "  ax = pyplot.gca()\n",
        "\n",
        "  for box in yhat['rois']:\n",
        "    y1, x1, y2, x2 = box\n",
        "    width, height = x2 - x1, y2 - y1\n",
        "\n",
        "    rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n",
        "    ax.add_patch(rect)\n",
        "  \n",
        "  for coors in face_to_nearest_text:\n",
        "    face_x, face_y = coors[0][0], coors[0][1]\n",
        "    text_x, text_y = coors[1][0], coors[1][1]\n",
        "\n",
        "    length_x = abs(face_x - text_x)\n",
        "    length_y = abs(face_y - text_y)\n",
        "\n",
        "    arrow = Arrow(face_x, face_y, length_x, length_y)\n",
        "    ax.add_patch(arrow)\n",
        "\n",
        "  pyplot.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "uNhxoCJ3wSvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def assoc_face_text(dataset, xml_file, image_id):\n",
        "  boxes, _, _ = dataset.extract_boxes(xml_file)\n",
        "  face_centers = []\n",
        "  text_centers = []\n",
        "\n",
        "  for box in boxes:\n",
        "    #find centers of faces and text\n",
        "    if \"face\" in box:\n",
        "      face_x = (int(box[0]) + int(box[2]))//2 #average of xmin and xmax\n",
        "      face_y = (int(box[1]) + int(box[3]))//2 #average of ymin and ymax\n",
        "      face_centers.append([face_x, face_y])\n",
        "    elif \"text\" in box:\n",
        "      text_x = (int(box[0]) + int(box[2]))//2 #average of xmin and xmax\n",
        "      text_y = (int(box[1]) + int(box[3]))//2 #average of ymin and ymax\n",
        "      text_centers.append([text_x, text_y])\n",
        "\n",
        "  face_to_nearest_text = []\n",
        "\n",
        "  for face in face_centers:\n",
        "    nearest_text = text_centers[0]\n",
        "    distance_x = abs(face[0] - nearest_text[0])\n",
        "    distance_y = abs(face[1] - nearest_text[1])\n",
        "    shortest_distance = math.sqrt(distance_x**2 + distance_y**2)\n",
        "\n",
        "    for text in text_centers:\n",
        "      distance_x = abs(face[0] - text[0])\n",
        "      distance_y = abs(face[1] - text[1])\n",
        "      distance = math.sqrt(distance_x**2 + distance_y**2)\n",
        "      if distance < shortest_distance:\n",
        "        shortest_distance = distance\n",
        "        nearest_text = text\n",
        "    \n",
        "    face_to_nearest_text.append([face, nearest_text]) #coordinates\n",
        "  \n",
        "  return face_to_nearest_text"
      ],
      "outputs": [],
      "metadata": {
        "id": "3VKoSdRckJJu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dataset = test_set\n",
        "# dataset = train_set + test_set\n",
        "\n",
        "for book in p.books:\n",
        "  images_dir = root_dir + \"images/\" + book + \"/\"\n",
        "  annotations_dir = root_dir + \"/annotations/\" + book + \"/\"\n",
        "\n",
        "  for img in listdir(images_dir):\n",
        "    image_id = int(img[:-4])\n",
        "    xml_file = annotations_dir + \"new_\" + book + str(image_id) + \".xml\"\n",
        "\n",
        "    tree = ElementTree.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    faces = []\n",
        "    texts = []\n",
        "\n",
        "    for face in root.findall(\".//face\"):\n",
        "      faces.append(face)\n",
        "    \n",
        "    for text in root.findall(\".//text\"):\n",
        "      texts.append(text)\n",
        "    \n",
        "    if len(faces) < 1: #if there are no faces\n",
        "      continue\n",
        "    \n",
        "    if len(texts) < 1: #if there are no texts\n",
        "      continue\n",
        "\n",
        "  face_to_nearest_text = assoc_face_text(dataset, xml_file, image_id)\n",
        "  arrow_face_text(dataset, image_id, group)\n",
        "\n",
        "  # for group in face_to_nearest_text:\n",
        "  #   arrow_face_text(dataset, image_id, group)\n",
        "\n",
        "  break"
      ],
      "outputs": [],
      "metadata": {
        "id": "89CXAA7tmiMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detect in New Photos"
      ],
      "metadata": {
        "id": "i72u9JmBQu7r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def plot_actual_vs_predicted(dataset, model, cfg, n_images=5):\n",
        "  for i in range(n_images):\n",
        "    image = dataset.load_image(i)\n",
        "    mask, _ = dataset.load_mask(i)\n",
        "    scaled_image = mold_image(image, cfg)\n",
        "    sample = expand_dims(scaled_image, 0)\n",
        "\n",
        "    yhat = model.detect(sample, verbose=0)[0]\n",
        "\n",
        "    pyplot.subplot(n_images, 2, i*2+1)\n",
        "    pyplot.imshow(image)\n",
        "    pyplot.title('Actual')\n",
        "\n",
        "    for j in range(mask.shape[2]):\n",
        "      pyplot.imshow(mask[:, :, j], cmap='gray', alpha=0.3)\n",
        "\n",
        "    pyplot.subplot(n_images, 2, i*2+2)\n",
        "    pyplot.imshow(image)\n",
        "    pyplot.title('Predicted')\n",
        "    ax = pyplot.gca()\n",
        "\n",
        "    for box in yhat['rois']:\n",
        "      y1, x1, y2, x2 = box\n",
        "      width, height = x2 - x1, y2 - y1\n",
        "\n",
        "      rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n",
        "      ax.add_patch(rect)\n",
        "\n",
        "  pyplot.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "OxF2V7GfoMJX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = MaskRCNN(mode='inference', model_dir='./', config=cfg)\n",
        "model_path = '/content/drive/MyDrive/NRP/Project/FaceSpeech/model_2.h5'\n",
        "model.load_weights(model_path, by_name=True)\n",
        "\n",
        "# plot_actual_vs_predicted(train_set, model, cfg)\n",
        "plot_actual_vs_predicted(test_set, model, cfg)"
      ],
      "outputs": [],
      "metadata": {
        "id": "kghylYqvhKVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Credits\n",
        "\n",
        "[Kangaroo](https://machinelearningmastery.com/how-to-train-an-object-detection-model-with-keras/)\n",
        "\n",
        "[Matterport Mask RCNN](https://github.com/matterport/Mask_RCNN)\n",
        "\n",
        "[Manga109](http://www.manga109.org/en/index.html)"
      ],
      "metadata": {
        "id": "mtu7lTcqpV_9"
      }
    }
  ]
}